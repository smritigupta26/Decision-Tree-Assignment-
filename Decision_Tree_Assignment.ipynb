{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree Assignment"
      ],
      "metadata": {
        "id": "Gh8CWfyB7YL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ASSIGNMENT QUESTIONS ANSWERS"
      ],
      "metadata": {
        "id": "DqRbgUBW7YGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "Answer 1 : A **Decision Tree** is a supervised learning algorithm used for both classification and regression. In the context of **classification**, it works by recursively splitting the dataset into subsets based on feature values, forming a tree-like structure. Each **internal node** represents a decision based on a feature (e.g., \"Is age > 30?\"), each **branch** represents the outcome of that decision, and each **leaf node** represents a class label.\n",
        "\n",
        "The algorithm selects features to split using measures like **Gini Index** or **Entropy (Information Gain)**, aiming to create the most homogeneous groups possible.\n",
        "\n",
        "For example, in a dataset predicting whether a warrior from the Mahabharata joins the Pandavas or Kauravas, features might include **loyalty**, **family ties**, and **dharma principles**. A rule could be: *If loyalty = Bhishma‚Äôs vow ‚Üí Kauravas; if guided by dharma like Arjuna ‚Üí Pandavas.*\n",
        "\n",
        "Decision Trees are easy to interpret but prone to **overfitting**, often addressed using pruning or ensemble methods like Random Forests.\n"
      ],
      "metadata": {
        "id": "9-nEFnF47YDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "6W9NSCcB8qti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "\n",
        "Answer 2: Gini Impurity and Entropy are two common measures used in Decision Trees to determine the quality of a split at each node.\n",
        "\n",
        "**Gini Impurity** measures the probability of incorrectly classifying a randomly chosen element if it were labeled according to the distribution of classes in the node. It is calculated as:\n",
        "\n",
        "$$\n",
        "Gini = 1 - \\sum p_i^2\n",
        "$$\n",
        "\n",
        "where $p_i$ is the probability of class $i$. A Gini of 0 means the node is pure (all samples belong to one class).\n",
        "\n",
        "**Entropy**, derived from information theory, measures the disorder or uncertainty in a dataset. It is given by:\n",
        "\n",
        "$$\n",
        "Entropy = - \\sum p_i \\log_2(p_i)\n",
        "$$\n",
        "\n",
        "A lower entropy value indicates a purer node, while higher entropy shows more randomness.\n",
        "\n",
        "**Impact on Splits:**\n",
        "Decision Trees use these measures to select the best split. The algorithm tries to reduce impurity after each split. Gini tends to favor larger partitions with dominant classes, while Entropy is more sensitive to class distribution.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "1. If a node has samples $[50 Yes, 0 No]$, both Gini and Entropy = 0 (pure).\n",
        "2. For $[25 Yes, 25 No]$: Gini = 0.5, Entropy = 1 (high impurity).\n",
        "3. For $[40 Yes, 10 No]$: Gini = 0.32, Entropy ‚âà 0.72 (moderate impurity).\n",
        "\n",
        "Thus, both guide the tree toward splits that increase purity and improve classification.\n"
      ],
      "metadata": {
        "id": "lvz6tRey8H98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "lHTuKagx7YAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "\n",
        "Answer 3 : **Pre-Pruning** and **Post-Pruning** are techniques to prevent overfitting in Decision Trees by controlling their complexity.\n",
        "\n",
        "**Pre-Pruning (Early Stopping):**\n",
        "In pre-pruning, the tree growth is restricted before it becomes too deep. Constraints such as maximum depth, minimum samples per leaf, or minimum information gain are applied during training. This prevents the model from splitting nodes that do not significantly improve prediction.\n",
        "\n",
        "* *Example:* If we set a maximum depth of 3 while predicting whether warriors in the Mahabharata would win based on their weapons and allies, the tree will stop after 3 levels, avoiding overly complex splits.\n",
        "* *Advantage:* It saves computation time and reduces overfitting by keeping the tree simpler.\n",
        "\n",
        "**Post-Pruning (Pruning After Full Growth):**\n",
        "In post-pruning, the tree is first grown to its maximum possible depth, and then non-essential branches are removed based on validation performance. The pruning step eliminates nodes that do not contribute significantly to accuracy.\n",
        "\n",
        "* *Example:* A fully grown tree predicting battle outcomes might create very specific rules like *‚ÄúIf warrior has bow + ally is Krishna + battlefield is Kurukshetra‚Äù*. Post-pruning would remove such overly specific branches if they don‚Äôt generalize well.\n",
        "* *Advantage:* It usually results in better accuracy since the tree explores all splits first and then trims unhelpful complexity.\n",
        "\n",
        "üëâ In short, pre-pruning controls growth early, while post-pruning refines a fully grown tree.\n"
      ],
      "metadata": {
        "id": "IxjstjVQ8vpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "OqeXGVMi7X87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "Answer 4 :Information Gain is a metric used in decision tree learning to determine the best way to split the data at each node. It quantifies how much a particular feature reduces uncertainty (or entropy) about the target variable. Essentially, it measures how much more predictable the data becomes after splitting on a specific feature.\n",
        "How it works:\n",
        "\n",
        "1. Entropy:\n",
        "\n",
        "Entropy measures the impurity or randomness of a dataset. A node with high entropy has a mix of different classes, while a node with low entropy has mostly one class.\n",
        "\n",
        "2. Information Gain Calculation:\n",
        "\n",
        "Information Gain is calculated by subtracting the weighted average entropy of the child nodes (created by the split) from the entropy of the parent node.\n",
        "\n",
        "3. Choosing the Best Split:\n",
        "\n",
        "The feature with the highest Information Gain is chosen as the best split because it leads to the most significant reduction in uncertainty and creates more homogenous child nodes.\n",
        "\n",
        "Example:\n",
        "Imagine a decision tree trying to predict whether someone will play tennis based on weather conditions. One feature is \"Outlook,\" which can be sunny, overcast, or rainy. Another feature is \"Humidity,\" which can be high or normal.\n",
        "If splitting on \"Outlook\" results in three child nodes: one with mostly \"play tennis\" outcomes, another with mostly \"no play tennis,\" and a third with a mix, the Information Gain will be calculated. If splitting on \"Humidity\" results in two child nodes, one with all \"play tennis\" and the other with mostly \"no play tennis\", the Information Gain of splitting on Humidity will likely be higher because it created more pure child nodes.\n",
        "\n",
        "\n",
        "*Why it's important:*\n",
        "\n",
        "By selecting features with the highest Information Gain, the decision tree aims to create branches that lead to increasingly pure (less uncertain) subsets of data, ultimately leading to more accurate classifications. In simpler terms, it helps the model make the most informed decisions at each step of the tree construction"
      ],
      "metadata": {
        "id": "7NXM2c7K7X5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "jQMqOaTf7X1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Answer 5 : Decision trees are versatile tools with applications across diverse fields, offering both advantages and limitations. Common uses include loan approval in banking, medical diagnosis, customer churn prediction, and fraud detection. Their strengths lie in interpretability and ease of understanding, while limitations include susceptibility to overfitting and potential instability with small changes in data.\n",
        "\n",
        "\n",
        "###*Real-world applications:*\n",
        "\n",
        "1. Banking:\n",
        "Decision trees help assess loan applications based on factors like credit score and income, aiding in quick and reliable approval decisions.\n",
        "\n",
        "2. Healthcare:\n",
        "They assist in disease diagnosis, such as predicting diabetes based on clinical data like glucose levels.\n",
        "\n",
        "3. Marketing:\n",
        "Businesses use them to predict customer churn (likelihood of leaving) based on behavior patterns and purchase history.\n",
        "\n",
        "4. Fraud Detection:\n",
        "They help identify fraudulent activities, like credit card fraud, by analyzing transaction patterns.\n",
        "\n",
        "###*Advantages:*\n",
        "\n",
        "1. Interpretability: Decision trees are easy to understand and visualize, making the decision-making process transparent.\n",
        "\n",
        "2. Handles diverse data: They can handle both numerical and categorical data.\n",
        "Feature selection: They help identify the most relevant attributes for prediction.\n",
        "\n",
        "3. Relatively low data preparation: They require less data preparation compared to some other machine learning algorithms.\n",
        "\n",
        "###*Limitations:*\n",
        "\n",
        "1. Overfitting:\n",
        "Large decision trees can be prone to overfitting the training data, leading to poor generalization on unseen data.\n",
        "\n",
        "2. Instability:\n",
        "Slight changes in the training data can lead to a significantly different tree structure.\n",
        "\n",
        "4. Not suitable for very complex relationships:\n",
        "Decision trees might struggle with highly complex, non-linear relationships in data.\n",
        "\n"
      ],
      "metadata": {
        "id": "QkziQqVu9zx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "8ClyWGMb91c6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "\n",
        "‚óè Load the Iris Dataset\n",
        "\n",
        "‚óè Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "‚óè Print the model‚Äôs accuracy and feature importances\n"
      ],
      "metadata": {
        "id": "2DjydHn891Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Decision Tree Classifier with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vr522MU_XHK",
        "outputId": "a162e906-08ba-425b-8129-a9d39be63154"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "bnEXsM_AA4yb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "‚óè Load the Iris Dataset\n",
        "\n",
        "‚óè Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n"
      ],
      "metadata": {
        "id": "uVXoJApm91Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Decision Tree with max_depth=3\n",
        "dt_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dt_limited.fit(X_train, y_train)\n",
        "y_pred_limited = dt_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Train a fully grown Decision Tree (no max_depth restriction)\n",
        "dt_full = DecisionTreeClassifier(random_state=42)\n",
        "dt_full.fit(X_train, y_train)\n",
        "y_pred_full = dt_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_limited:.4f}\")\n",
        "print(f\"Accuracy with fully-grown tree: {accuracy_full:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjB_ZADtAOam",
        "outputId": "62a73f98-c041-4706-d1d0-a501dee96ea7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0000\n",
            "Accuracy with fully-grown tree: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "_S3tH3QnA65b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "‚óè Load the California Housing dataset from sklearn\n",
        "\n",
        "‚óè Train a Decision Tree Regressor\n",
        "\n",
        "‚óè Print the Mean Squared Error (MSE) and feature importances\n"
      ],
      "metadata": {
        "id": "L8E1joh991Pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# Feature importances\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': housing.feature_names,\n",
        "    'Importance': regressor.feature_importances_\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importances)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f75OlkcAwcO",
        "outputId": "63ce9db1-e59d-47ce-d894-de46306a03cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.495235205629094\n",
            "\n",
            "Feature Importances:\n",
            "      Feature  Importance\n",
            "0      MedInc    0.528509\n",
            "5    AveOccup    0.130838\n",
            "6    Latitude    0.093717\n",
            "7   Longitude    0.082902\n",
            "2    AveRooms    0.052975\n",
            "1    HouseAge    0.051884\n",
            "4  Population    0.030516\n",
            "3   AveBedrms    0.028660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "svKETmDg91JJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "‚óè Load the Iris Dataset\n",
        "\n",
        "‚óè Tune the Decision Tree‚Äôs max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "\n",
        "‚óè Print the best parameters and the resulting model accuracy\n"
      ],
      "metadata": {
        "id": "auWjIwzOBCeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Tune Decision Tree max_depth and min_samples_split using GridSearchCV\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'max_depth': [None, 3, 5, 7, 10],  # None means no limit\n",
        "    'min_samples_split': [2, 5, 10, 20]\n",
        "}\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 3. Print the best parameters and the resulting model accuracy\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Get the best estimator (model)\n",
        "best_dt_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_dt_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the best model on the test set: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jygXP9WmBL71",
        "outputId": "1a48456d-8ff7-47c3-dda6-07cf94260736"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found by GridSearchCV:\n",
            "{'max_depth': None, 'min_samples_split': 10}\n",
            "Accuracy of the best model on the test set: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "2lR0wzN6Bd6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you‚Äôre working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "‚óè Handle the missing values\n",
        "\n",
        "‚óè Encode the categorical features\n",
        "\n",
        "‚óè Train a Decision Tree model\n",
        "\n",
        "‚óè Tune its hyperparameters\n",
        "\n",
        "‚óè Evaluate its performance\n",
        "\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "ANSWER 10. Here‚Äôs a structured **step-by-step explanation** tailored for your question:\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 1: Handle Missing Values**\n",
        "\n",
        "* **Identify missing data**: Use `df.isnull().sum()` to see which features have missing values.\n",
        "* **Numerical features**: Replace missing values with the **mean** or **median** (depending on skewness). Example: `SimpleImputer(strategy=\"median\")`.\n",
        "* **Categorical features**: Replace missing values with the **most frequent value** or introduce a new category `\"Unknown\"`.\n",
        "* Business note: Proper handling ensures no patient records are discarded unnecessarily, keeping the dataset large and representative.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Encode Categorical Features**\n",
        "\n",
        "* **Label Encoding**: For ordinal categories (e.g., \"Mild\", \"Moderate\", \"Severe\").\n",
        "* **One-Hot Encoding**: For nominal categories (e.g., \"Blood Type: A, B, AB, O\"). Example: `OneHotEncoder(handle_unknown='ignore')`.\n",
        "* Business note: Correct encoding lets the model understand non-numeric patient details like gender, region, or symptoms.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Train a Decision Tree Model**\n",
        "\n",
        "* Split data into **training (80%)** and **test (20%)** sets using `train_test_split`.\n",
        "* Initialize a `DecisionTreeClassifier(random_state=42)` and train it on the processed data.\n",
        "* Business note: Decision Trees are interpretable, allowing doctors and stakeholders to understand the reasoning behind predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 4: Tune Hyperparameters**\n",
        "\n",
        "* Use **GridSearchCV** or **RandomizedSearchCV** to tune:\n",
        "\n",
        "  * `max_depth` (to prevent overfitting)\n",
        "  * `min_samples_split` (controls minimum patients per split)\n",
        "  * `min_samples_leaf` (ensures stability)\n",
        "  * `criterion` (\"gini\" or \"entropy\")\n",
        "* Example parameter grid:\n",
        "\n",
        "  ```python\n",
        "  param_grid = {\n",
        "      'max_depth': [3, 5, 7, None],\n",
        "      'min_samples_split': [2, 5, 10],\n",
        "      'min_samples_leaf': [1, 2, 4],\n",
        "      'criterion': ['gini', 'entropy']\n",
        "  }\n",
        "  ```\n",
        "* Business note: Tuning ensures the model is both **accurate and generalizable** for new patients.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 5: Evaluate Performance**\n",
        "\n",
        "* Use metrics beyond accuracy:\n",
        "\n",
        "  * **Precision & Recall** (important if false negatives are dangerous).\n",
        "  * **ROC-AUC Score** (to measure discrimination power).\n",
        "  * **Confusion Matrix** (to analyze true vs. false predictions).\n",
        "* Apply **cross-validation** for robust evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "### **Business Value in Real-World Setting**\n",
        "\n",
        "* **Early disease detection** ‚Üí Helps doctors flag high-risk patients sooner.\n",
        "* **Decision support** ‚Üí Model explanations (feature importance) guide doctors on key risk factors.\n",
        "* **Resource optimization** ‚Üí Hospitals can prioritize testing and treatments for patients most likely at risk.\n",
        "* **Cost savings & patient safety** ‚Üí Reduces unnecessary tests for low-risk patients, ensuring faster, targeted care.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dx2QSrjj91Fc"
      }
    }
  ]
}